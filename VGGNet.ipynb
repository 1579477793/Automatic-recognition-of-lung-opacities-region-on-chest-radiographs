{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG_loss.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CoIJ9Y3Ep8Os","colab_type":"code","outputId":"f5f162ec-46b3-4e8d-ad30-4baee99c8ca5","executionInfo":{"status":"ok","timestamp":1559876916698,"user_tz":-480,"elapsed":8978,"user":{"displayName":"Zheng Sun","photoUrl":"","userId":"15433979753103167691"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","import os\n","# os.chdir(\"drive/20190424\") \n","os.chdir(\"drive/BME_lung\") \n","! ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["fuse: mountpoint is not empty\n","fuse: if you are sure this is safe, use the 'nonempty' mount option\n"," drive\t\t\t\t        ResNet_runloss.ipynb（副本）\n","'GCP Credits Request Link - RSNA.txt'   stage_2_detailed_class_info.csv\n"," multi0605_layer1622_best_model.h5      stage_2_sample_submission.csv\n"," multi_layer1623_best_model.h5\t        stage_2_test_images\n"," multi_layer_best_model.h5\t        stage_2_train_images\n"," multi_layer_best_modelResNetLoss.h5    stage_2_train_labels.csv\n"," ResNet_loss_model.png\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iS8apmmPqR0D","colab_type":"code","outputId":"8464bc7f-f066-4d49-b016-685d44763d86","executionInfo":{"status":"ok","timestamp":1559822425816,"user_tz":-480,"elapsed":5102,"user":{"displayName":"Zheng Sun","photoUrl":"","userId":"15433979753103167691"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["! pip install pydicom"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (1.2.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qkCYNVYqqVO_","colab_type":"code","colab":{}},"source":["import os\n","import csv\n","import random\n","import pydicom\n","import numpy as np\n","import pandas as pd\n","from skimage import io\n","from skimage import measure\n","from skimage.transform import resize\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWFTeqZmqZMp","colab_type":"code","outputId":"25d10eb9-1b37-4abf-fd53-831e9212dddd","executionInfo":{"status":"ok","timestamp":1559876969353,"user_tz":-480,"elapsed":37965,"user":{"displayName":"Zheng Sun","photoUrl":"","userId":"15433979753103167691"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["folder = '../BME_lung/stage_2_train_images'\n","filenames = os.listdir(folder)\n","random.shuffle(filenames)\n","# split into train and validation filenames\n","n_valid_samples = 550\n","train_filenames = filenames[n_valid_samples:]  \n","valid_filenames = filenames[:n_valid_samples]\n","print('n train samples', len(train_filenames))  # 4700\n","print('n valid samples', len(valid_filenames))  # 550\n","n_train_samples = len(filenames) - n_valid_samples"],"execution_count":3,"outputs":[{"output_type":"stream","text":["n train samples 4700\n","n valid samples 550\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BYfoXoSEqcfJ","colab_type":"code","colab":{}},"source":["pneumonia_locations = {}\n","# load table\n","with open(os.path.join('../BME_lung/stage_2_train_labels.csv'), mode='r') as infile:\n","    # open reader\n","    reader = csv.reader(infile)\n","    #i=0\n","    # skip header\n","    next(reader, None)\n","    # loop through rows\n","    for rows in reader:\n","        # retrieve information\n","        filename = rows[0]\n","        location = rows[1:5]\n","        pneumonia = rows[5]\n","        # if row contains pneumonia add label to dictionary\n","        # which contains a list of pneumonia locations per filename\n","        if pneumonia == '1':\n","            #i = i+1\n","            # convert string to float to int\n","            location = [int(float(i)) for i in location]\n","            # save pneumonia location in dictionary\n","            if filename in pneumonia_locations:\n","                pneumonia_locations[filename].append(location)\n","                \n","            else:\n","                pneumonia_locations[filename] = [location]\n","    #print(i)            "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"epsEzS8fqjPv","colab_type":"code","colab":{}},"source":["class generator(keras.utils.Sequence):\n","    \n","    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=32, image_size=256, shuffle=True, augment=False, predict=False): # image_size=320 #0604\n","        self.folder = folder\n","        self.filenames = filenames\n","        self.pneumonia_locations = pneumonia_locations\n","        self.batch_size = batch_size\n","        self.image_size = image_size\n","        self.shuffle = shuffle\n","        self.augment = augment\n","        self.predict = predict\n","        self.on_epoch_end()\n","        \n","    def __load__(self, filename):\n","        ## load dicom file as numpy array\n","        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array #0604\n","        # img = pydicom.dcmread(os.path.join(self.folder, filename))\n","        # img.PixelRepresentation = 0\n","        ##print(img.dir())\n","        ##plt.imshow(img.pixel_array, cmap=plt.cm.bone)\n","        ##plt.show()\n","        ## create empty mask\n","        msk = np.zeros(img.shape)\n","        # get filename without extension\n","        filename = filename.split('.')[0]\n","        # if image contains pneumonia\n","        if filename in self.pneumonia_locations:\n","            # loop through pneumonia\n","            for location in self.pneumonia_locations[filename]:\n","                # add 1's at the location of the pneumonia\n","                x, y, w, h = location\n","                msk[y:y+h, x:x+w] = 1\n","        if self.augment and random.random() > 0.5:  # 0604\n","            img = np.fliplr(img)\n","            msk = np.fliplr(msk)\n","        # resize both image and mask\n","        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n","        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n","        # if augment then horizontal flip half the time\n","        #if self.augment and random.random() > 0.5: # 0604\n","        #    img = np.fliplr(img)\n","        #    msk = np.fliplr(msk)\n","        # add trailing channel dimension\n","        img = np.expand_dims(img, -1)\n","        msk = np.expand_dims(msk, -1)\n","        return img, msk\n","    \n","    def __loadpredict__(self, filename):\n","        # load dicom file as numpy array\n","        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n","        # resize image\n","        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n","        # add trailing channel dimension\n","        img = np.expand_dims(img, -1)\n","        return img\n","        \n","    def __getitem__(self, index):\n","        # select batch\n","        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n","        # predict mode: return images and filenames\n","        if self.predict:\n","            # load files\n","            imgs = [self.__loadpredict__(filename) for filename in filenames]\n","            # create numpy batch\n","            imgs = np.array(imgs)\n","            return imgs, filenames\n","        # train mode: return images and masks\n","        else:\n","            # load files\n","            items = [self.__load__(filename) for filename in filenames]\n","            # unzip images and masks\n","            imgs, msks = zip(*items)\n","            # create numpy batch\n","            imgs = np.array(imgs)\n","            msks = np.array(msks)\n","            return imgs, msks\n","        \n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            random.shuffle(self.filenames)\n","        \n","    def __len__(self):\n","        if self.predict:\n","            # return everything\n","            return int(np.ceil(len(self.filenames) / self.batch_size))\n","        else:\n","            # return full batches only\n","            return int(len(self.filenames) / self.batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z7IMclquqp4Q","colab_type":"code","colab":{}},"source":["def create_downsample(channels, inputs):\n","    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n","    # x = keras.layers.Conv2D(channels, 2, padding='same', use_bias=False)(x)\n","    x = keras.layers.MaxPool2D(2)(x)\n","    return x\n","\n","def create_resblock(channels, inputs):\n","    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n","    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n","    return x #keras.layers.add([x, inputs])\n","\n","def create_network(input_size, channels, n_blocks=2, depth=4):\n","    # input\n","    inputs = keras.Input(shape=(input_size, input_size, 1))\n","    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n","    # residual blocks\n","    for d in range(depth):\n","        channels = channels * 2\n","        x = create_downsample(channels, x)\n","        for b in range(n_blocks):\n","            x = create_resblock(channels, x)\n","    # output\n","    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n","    x = keras.layers.LeakyReLU(0)(x)\n","    # 0604\n","    # x = keras.layers.Conv2D(256, 1, activation=None)(x)\n","    # x = keras.layers.BatchNormalization(momentum=0.9)(x)\n","    # x = keras.layers.LeakyReLU(0)(x)\n","    #x = keras.layers.Conv2DTranspose(128, (8,8),(4,4),padding='same', activation=None)(x)\n","    #x = keras.layers.BatchNormalization(momentum=0.9)(x)\n","    #x = keras.layers.LeakyReLU(0)(x)\n","    \n","    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n","    outputs = keras.layers.UpSampling2D(2**(depth))(x)\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBuKrkViqyOA","colab_type":"code","outputId":"c59ca02c-e7d1-46ad-aaa9-014fa1a935c8","executionInfo":{"status":"ok","timestamp":1559877037904,"user_tz":-480,"elapsed":1487,"user":{"displayName":"Zheng Sun","photoUrl":"","userId":"15433979753103167691"}},"colab":{"base_uri":"https://localhost:8080/","height":1498}},"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","def iou_loss(y_true, y_pred):\n","    y_true = tf.reshape(y_true, [-1])\n","    y_pred = tf.reshape(y_pred, [-1])\n","    intersection = tf.reduce_sum(y_true * y_pred)\n","    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n","    return 1 - score\n","\n","# combine bce loss and iou loss\n","def iou_bce_loss(y_true, y_pred):\n","    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n","\n","# mean iou as a metric\n","def mean_iou(y_true, y_pred):\n","    y_pred = tf.round(y_pred)\n","    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n","    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n","    smooth = tf.ones(tf.shape(intersect))\n","    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n","\n","# create network and compiler\n","model = create_network(input_size=256, channels=32, n_blocks=2, depth=2)\n","model.compile(optimizer='adam',\n","              loss=iou_loss,\n","              metrics=[mean_iou])\n","\n","# cosine learning rate annealing\n","def cosine_annealing(x):\n","    lr = 0.001\n","    epochs = 25 #64\n","    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n","learning_rate = [tf.keras.callbacks.LearningRateScheduler(cosine_annealing)]\n","\n","\n","folder = '../BME_lung/stage_2_train_images'\n","train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)\n","valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False)\n","print(model.summary())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 256, 256, 1)       0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, 256, 256, 32)      288       \n","_________________________________________________________________\n","batch_normalization_v1 (Batc (None, 256, 256, 32)      128       \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 256, 256, 32)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 256, 256, 64)      2048      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","batch_normalization_v1_1 (Ba (None, 128, 128, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 128, 128, 64)      36864     \n","_________________________________________________________________\n","batch_normalization_v1_2 (Ba (None, 128, 128, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 128, 128, 64)      36864     \n","_________________________________________________________________\n","batch_normalization_v1_3 (Ba (None, 128, 128, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 128, 128, 64)      36864     \n","_________________________________________________________________\n","batch_normalization_v1_4 (Ba (None, 128, 128, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 128, 128, 64)      36864     \n","_________________________________________________________________\n","batch_normalization_v1_5 (Ba (None, 128, 128, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 128, 128, 128)     8192      \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","batch_normalization_v1_6 (Ba (None, 64, 64, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 64, 64, 128)       147456    \n","_________________________________________________________________\n","batch_normalization_v1_7 (Ba (None, 64, 64, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 64, 64, 128)       147456    \n","_________________________________________________________________\n","batch_normalization_v1_8 (Ba (None, 64, 64, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)    (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 64, 64, 128)       147456    \n","_________________________________________________________________\n","batch_normalization_v1_9 (Ba (None, 64, 64, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 64, 64, 128)       147456    \n","_________________________________________________________________\n","batch_normalization_v1_10 (B (None, 64, 64, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)   (None, 64, 64, 128)       0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 64, 64, 1)         129       \n","_________________________________________________________________\n","up_sampling2d (UpSampling2D) (None, 256, 256, 1)       0         \n","=================================================================\n","Total params: 751,905\n","Trainable params: 749,921\n","Non-trainable params: 1,984\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"guB0jzMwrvF2","colab_type":"code","outputId":"5062eb34-1b03-4eb8-f0a1-b99d8b26d0ca","executionInfo":{"status":"error","timestamp":1559877236904,"user_tz":-480,"elapsed":190530,"user":{"displayName":"Zheng Sun","photoUrl":"","userId":"15433979753103167691"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=learning_rate, epochs=25 )"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n"," 16/146 [==>...........................] - ETA: 24:25 - loss: 0.9306 - mean_iou: 0.0540"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-48733cc8da48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(output_generator, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# Returning `None` will trigger looping to stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}